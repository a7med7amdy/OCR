{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon  #make dialation, errosion, shapes\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "torch.set_printoptions(linewidth=120)\n",
    "import skimage.io as io\n",
    "import cv2 as cv2\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  DataSet ##################################################\n",
    "\n",
    "def cal_distance(x1, y1, x2, y2):\n",
    "    '''calculate the Euclidean distance'''\n",
    "    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "\n",
    "def move_points(vertices, index1, index2, r, coef):\n",
    "    '''move the two points to shrink edge\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        index1  : offset of point1\n",
    "        index2  : offset of point2\n",
    "        r       : [r1, r2, r3, r4] in paper\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        vertices: vertices where one edge has been shinked\n",
    "    '''\n",
    "    index1 = index1 % 4\n",
    "    index2 = index2 % 4\n",
    "    x1_index = index1 * 2 + 0\n",
    "    y1_index = index1 * 2 + 1\n",
    "    x2_index = index2 * 2 + 0\n",
    "    y2_index = index2 * 2 + 1\n",
    "\n",
    "    r1 = r[index1]\n",
    "    r2 = r[index2]\n",
    "    length_x = vertices[x1_index] - vertices[x2_index]\n",
    "    length_y = vertices[y1_index] - vertices[y2_index]\n",
    "    length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\n",
    "    if length > 1:\n",
    "        ratio = (r1 * coef) / length\n",
    "        vertices[x1_index] += ratio * (-length_x) \n",
    "        vertices[y1_index] += ratio * (-length_y) \n",
    "        ratio = (r2 * coef) / length\n",
    "        vertices[x2_index] += ratio * length_x \n",
    "        vertices[y2_index] += ratio * length_y\n",
    "    return vertices\t\n",
    "\n",
    "\n",
    "def shrink_poly(vertices, coef=0.3):\n",
    "    '''shrink the text region\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        v       : vertices of shrinked text region <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    r1 = min(cal_distance(x1,y1,x2,y2), cal_distance(x1,y1,x4,y4))\n",
    "    r2 = min(cal_distance(x2,y2,x1,y1), cal_distance(x2,y2,x3,y3))\n",
    "    r3 = min(cal_distance(x3,y3,x2,y2), cal_distance(x3,y3,x4,y4))\n",
    "    r4 = min(cal_distance(x4,y4,x1,y1), cal_distance(x4,y4,x3,y3))\n",
    "    r = [r1, r2, r3, r4]\n",
    "\n",
    "    # obtain offset to perform move_points() automatically\n",
    "    if cal_distance(x1,y1,x2,y2) + cal_distance(x3,y3,x4,y4) > \\\n",
    "       cal_distance(x2,y2,x3,y3) + cal_distance(x1,y1,x4,y4):\n",
    "        offset = 0 # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\n",
    "    else:\n",
    "        offset = 1 # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\n",
    "\n",
    "    v = vertices.copy()\n",
    "    v = move_points(v, 0 + offset, 1 + offset, r, coef)\n",
    "    v = move_points(v, 2 + offset, 3 + offset, r, coef)\n",
    "    v = move_points(v, 1 + offset, 2 + offset, r, coef)\n",
    "    v = move_points(v, 3 + offset, 4 + offset, r, coef)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_rotate_mat(theta):\n",
    "    '''positive theta value means rotate clockwise'''\n",
    "    return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n",
    "\n",
    "\n",
    "def rotate_vertices(vertices, theta, anchor=None):\n",
    "    '''rotate vertices around anchor\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        theta   : angle in radian measure\n",
    "        anchor  : fixed position during rotation\n",
    "    Output:\n",
    "        rotated vertices <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    v = vertices.reshape((4,2)).T\n",
    "    if anchor is None:\n",
    "        anchor = v[:,:1]\n",
    "    rotate_mat = get_rotate_mat(theta)\n",
    "    res = np.dot(rotate_mat, v - anchor)\n",
    "    return (res + anchor).T.reshape(-1)\n",
    "\n",
    "\n",
    "def get_boundary(vertices):\n",
    "    '''get the tight boundary around given vertices\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the boundary\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    x_min = min(x1, x2, x3, x4)\n",
    "    x_max = max(x1, x2, x3, x4)\n",
    "    y_min = min(y1, y2, y3, y4)\n",
    "    y_max = max(y1, y2, y3, y4)\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def cal_error(vertices):\n",
    "    '''default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n",
    "    calculate the difference between the vertices orientation and default orientation\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        err     : difference measure\n",
    "    '''\n",
    "    x_min, x_max, y_min, y_max = get_boundary(vertices)\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\n",
    "          cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max)\n",
    "    return err\n",
    "\n",
    "\n",
    "def find_min_rect_angle(vertices):\n",
    "    '''find the best angle to rotate poly and obtain min rectangle\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the best angle <radian measure>\n",
    "    '''\n",
    "    angle_interval = 1\n",
    "    angle_list = list(range(-90, 90, angle_interval))\n",
    "    area_list = []\n",
    "    for theta in angle_list: \n",
    "        rotated = rotate_vertices(vertices, theta / 180 * math.pi)\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n",
    "        temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\n",
    "                    (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\n",
    "        area_list.append(temp_area)\n",
    "\n",
    "    sorted_area_index = sorted(list(range(len(area_list))), key=lambda k : area_list[k])\n",
    "    min_error = float('inf')\n",
    "    best_index = -1\n",
    "    rank_num = 10\n",
    "    # find the best angle with correct orientation\n",
    "    for index in sorted_area_index[:rank_num]:\n",
    "        rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n",
    "        temp_error = cal_error(rotated)\n",
    "        if temp_error < min_error:\n",
    "            min_error = temp_error\n",
    "            best_index = index\n",
    "    return angle_list[best_index] / 180 * math.pi\n",
    "\n",
    "\n",
    "def is_cross_text(start_loc, length, vertices):\n",
    "    '''check if the crop image crosses text regions\n",
    "    Input:\n",
    "        start_loc: left-top position\n",
    "        length   : length of crop image\n",
    "        vertices : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "    Output:\n",
    "        True if crop image crosses text region\n",
    "    '''\n",
    "    if vertices.size == 0:\n",
    "        return False\n",
    "    start_w, start_h = start_loc\n",
    "    a = np.array([start_w, start_h, start_w + length, start_h, \\\n",
    "          start_w + length, start_h + length, start_w, start_h + length]).reshape((4,2))\n",
    "    p1 = Polygon(a).convex_hull\n",
    "    for vertice in vertices:\n",
    "        p2 = Polygon(vertice.reshape((4,2))).convex_hull\n",
    "        inter = p1.intersection(p2).area\n",
    "        if 0.01 <= inter / p2.area <= 0.99: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def crop_img(img, vertices, labels, length):\n",
    "    '''crop img patches to obtain batch and augment\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels      : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        length      : length of cropped image region\n",
    "    Output:\n",
    "        region      : cropped image region\n",
    "        new_vertices: new vertices in cropped region\n",
    "    '''\n",
    "    h, w = img.height, img.width\n",
    "    # confirm the shortest side of image >= length\n",
    "    if h >= w and w < length:\n",
    "        img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n",
    "    elif h < w and h < length:\n",
    "        img = img.resize((int(w * length / h), length), Image.BILINEAR)\n",
    "    ratio_w = img.width / w\n",
    "    ratio_h = img.height / h\n",
    "    assert(ratio_w >= 1 and ratio_h >= 1)\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[0,2,4,6]] = vertices[:,[0,2,4,6]] * ratio_w\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * ratio_h\n",
    "\n",
    "    # find random position\n",
    "    remain_h = img.height - length\n",
    "    remain_w = img.width - length\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag and cnt < 1000:\n",
    "        cnt += 1\n",
    "        start_w = int(np.random.rand() * remain_w)\n",
    "        start_h = int(np.random.rand() * remain_h)\n",
    "        flag = is_cross_text([start_w, start_h], length, new_vertices[labels==1,:])\n",
    "    box = (start_w, start_h, start_w + length, start_h + length)\n",
    "    region = img.crop(box)\n",
    "    if new_vertices.size == 0:\n",
    "        return region, new_vertices\t\n",
    "\n",
    "    new_vertices[:,[0,2,4,6]] -= start_w\n",
    "    new_vertices[:,[1,3,5,7]] -= start_h\n",
    "    return region, new_vertices\n",
    "\n",
    "\n",
    "def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\n",
    "    '''get rotated locations of all pixels for next stages\n",
    "    Input:\n",
    "        rotate_mat: rotatation matrix\n",
    "        anchor_x  : fixed x position\n",
    "        anchor_y  : fixed y position\n",
    "        length    : length of image\n",
    "    Output:\n",
    "        rotated_x : rotated x positions <numpy.ndarray, (length,length)>\n",
    "        rotated_y : rotated y positions <numpy.ndarray, (length,length)>\n",
    "    '''\n",
    "    x = np.arange(length)\n",
    "    y = np.arange(length)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\n",
    "                                                   np.array([[anchor_x], [anchor_y]])\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y\n",
    "\n",
    "\n",
    "def adjust_height(img, vertices, ratio=0.2):\n",
    "    '''adjust height of image to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        ratio       : height changes in [0.8, 1.2]\n",
    "    Output:\n",
    "        img         : adjusted PIL Image\n",
    "        new_vertices: adjusted vertices\n",
    "    '''\n",
    "    ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\n",
    "    old_h = img.height\n",
    "    new_h = int(np.around(old_h * ratio_h))\n",
    "    img = img.resize((img.width, new_h), Image.BILINEAR)\n",
    "\n",
    "    new_vertices = vertices.copy()\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * (new_h / old_h)\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def rotate_img(img, vertices, angle_range=10):\n",
    "    '''rotate image [-10, 10] degree to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        angle_range : rotate range\n",
    "    Output:\n",
    "        img         : rotated PIL Image\n",
    "        new_vertices: rotated vertices\n",
    "    '''\n",
    "    center_x = (img.width - 1) / 2\n",
    "    center_y = (img.height - 1) / 2\n",
    "    angle = angle_range * (np.random.rand() * 2 - 1)\n",
    "    img = img.rotate(angle, Image.BILINEAR)\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        new_vertices[i,:] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x],[center_y]]))\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def get_score_geo(img, vertices, labels, scale, length):\n",
    "    '''generate score gt and geometry gt\n",
    "    Input:\n",
    "        img     : PIL Image\n",
    "        vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        scale   : feature map / image\n",
    "        length  : image length\n",
    "    Output:\n",
    "        score gt, geo gt, ignored\n",
    "    '''\n",
    "    score_map   = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n",
    "    geo_map     = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\n",
    "    ignored_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n",
    "\n",
    "    index = np.arange(0, length, int(1/scale))\n",
    "    index_x, index_y = np.meshgrid(index, index)\n",
    "    ignored_polys = []\n",
    "    polys = []\n",
    "\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        if labels[i] == 0:\n",
    "            ignored_polys.append(np.around(scale * vertice.reshape((4,2))).astype(np.int32))\n",
    "            continue\t\t\n",
    "\n",
    "        poly = np.around(scale * shrink_poly(vertice).reshape((4,2))).astype(np.int32) # scaled & shrinked\n",
    "        polys.append(poly)\n",
    "        temp_mask = np.zeros(score_map.shape[:-1], np.float32)\n",
    "        cv2.fillPoly(temp_mask, [poly], 1)\n",
    "\n",
    "        theta = find_min_rect_angle(vertice)\n",
    "        rotate_mat = get_rotate_mat(theta)\n",
    "\n",
    "        rotated_vertices = rotate_vertices(vertice, theta)\n",
    "        x_min, x_max, y_min, y_max = get_boundary(rotated_vertices)\n",
    "        rotated_x, rotated_y = rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length)\n",
    "\n",
    "        d1 = rotated_y - y_min\n",
    "        d1[d1<0] = 0\n",
    "        d2 = y_max - rotated_y\n",
    "        d2[d2<0] = 0\n",
    "        d3 = rotated_x - x_min\n",
    "        d3[d3<0] = 0\n",
    "        d4 = x_max - rotated_x\n",
    "        d4[d4<0] = 0\n",
    "        geo_map[:,:,0] += d1[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,1] += d2[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,2] += d3[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,3] += d4[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,4] += theta * temp_mask\n",
    "\n",
    "    cv2.fillPoly(ignored_map, ignored_polys, 1)\n",
    "    cv2.fillPoly(score_map, polys, 1)\n",
    "    return torch.Tensor(score_map).permute(2,0,1), torch.Tensor(geo_map).permute(2,0,1), torch.Tensor(ignored_map).permute(2,0,1)\n",
    "\n",
    "\n",
    "def extract_vertices(lines):\n",
    "    '''extract vertices info from txt lines\n",
    "    Input:\n",
    "        lines   : list of string info\n",
    "    Output:\n",
    "        vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "    '''\n",
    "    labels = []\n",
    "    vertices = []\n",
    "    for line in lines:\n",
    "        vertices.append(list(map(int,line.rstrip('\\n').lstrip('\\ufeff').split(',')[:8])))\n",
    "        label = 0 if '###' in line else 1\n",
    "        labels.append(label)\n",
    "    return np.array(vertices), np.array(labels)\n",
    "\n",
    "\n",
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, img_path, gt_path, scale=0.25, length=512):\n",
    "        super(custom_dataset, self).__init__()\n",
    "        self.img_files = [os.path.join(img_path, img_file) for img_file in sorted(os.listdir(img_path))]\n",
    "        self.gt_files  = [os.path.join(gt_path, gt_file) for gt_file in sorted(os.listdir(gt_path))]\n",
    "        self.scale = scale\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with open(self.gt_files[index], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        vertices, labels = extract_vertices(lines)\n",
    "\n",
    "        img = Image.open(self.img_files[index])\n",
    "        img, vertices = adjust_height(img, vertices) \n",
    "        img, vertices = rotate_img(img, vertices)\n",
    "        img, vertices = crop_img(img, vertices, labels, self.length) \n",
    "        transform = transforms.Compose([transforms.ColorJitter(0.5, 0.5, 0.5, 0.25), \\\n",
    "                                        transforms.ToTensor(), \\\n",
    "                                        transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "\n",
    "        score_map, geo_map, ignored_map = get_score_geo(img, vertices, labels, self.scale, self.length)\n",
    "        return transform(img), score_map, geo_map, ignored_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################   model ########################################################3\n",
    "\n",
    "# cfg if not pretrained extractor \"VGG\"\n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "#implementation of VGG\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#in extractor in the paper they adopted the VGG16 as its advantage of feature extarctor\n",
    "#between pool of 2 to 5\n",
    "class extractor(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(extractor, self).__init__()\n",
    "#         vgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n",
    "        if pretrained:\n",
    "#             vgg16_bn.load_state_dict(torch.load('./pths/vgg16_bn-6c64b313.pth'))\n",
    "#             vgg16_bn.load_state_dict(torch.load('https://download.pytorch.org/models/vgg16-397923af.pth'))\n",
    "#             vgg16_bn.state_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/vgg16-397923af.pth')\n",
    "            vgg16 = models.vgg16(pretrained=True)\n",
    "    \n",
    "            \n",
    "        self.features = vgg16.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        \n",
    "        #explaination\n",
    "        for m in self.features:\n",
    "            x = m(x)\n",
    "            if isinstance(m, nn.MaxPool2d):\n",
    "                out.append(x)\n",
    "        return out[1:]\n",
    "\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(merge, self).__init__()\n",
    "        \n",
    "        #384*2+256= 1024 as the concatenation of the f2 + unpool(opposite of max pool) of f1\n",
    "        self.conv1 = nn.Conv2d(1024, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #128*2 + 128 =384 (concat)\n",
    "        self.conv3 = nn.Conv2d(384, 64, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(192, 32, 1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "#         The self.modules() method returns an iterable to the many layers or “modules” defined in the model class.\n",
    "#         This particular piece of code is using that self.modules() iterable to initialize the weights of the\n",
    "#         different layers present in the model. isinstance() checks if the particular layer “m” is an instance of\n",
    "#         a conv2d or linear or conv3d layer etc. and initializes the weights accordingly.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward of the paper \n",
    "#         1-get the f1 (x[3]), unpool by 2 (*2) but with interpolation to make unpool\n",
    "#         2- concat the f1 with f2 (x[3], x[2])\n",
    "#         3-feed the output to the first bulk which is conv and so and so of the forward\n",
    "#         4-make the same thing for the next bulks\n",
    "        \n",
    "        y = F.interpolate(x[3], scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[2]), 1)\n",
    "        y = self.relu1(self.bn1(self.conv1(y)))\n",
    "        y = self.relu2(self.bn2(self.conv2(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[1]), 1)\n",
    "        y = self.relu3(self.bn3(self.conv3(y)))\n",
    "        y = self.relu4(self.bn4(self.conv4(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[0]), 1)\n",
    "        y = self.relu5(self.bn5(self.conv5(y)))\n",
    "        y = self.relu6(self.bn6(self.conv6(y)))\n",
    "\n",
    "        y = self.relu7(self.bn7(self.conv7(y)))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class output(nn.Module):\n",
    "    def __init__(self, scope=512):\n",
    "        super(output, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.conv2 = nn.Conv2d(32, 4, 1)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.conv3 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        #what's meant by self.scope\n",
    "        self.scope = 512\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = self.sigmoid1(self.conv1(x))\n",
    "        loc   = self.sigmoid2(self.conv2(x)) * self.scope\n",
    "        angle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n",
    "        geo   = torch.cat((loc, angle), 1) \n",
    "        return score, geo\n",
    "\n",
    "\n",
    "class EAST(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EAST, self).__init__()\n",
    "        self.extractor = extractor(pretrained)\n",
    "        self.merge     = merge()\n",
    "        self.output    = output()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.merge(self.extractor(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################  Loss calculations ########################### \n",
    "\n",
    "\n",
    "# IOU\n",
    "def get_dice_loss(gt_score, pred_score):\n",
    "    inter = torch.sum(gt_score * pred_score)\n",
    "    union = torch.sum(gt_score) + torch.sum(pred_score) + 1e-5\n",
    "    return 1. - (2 * inter / union)\n",
    "\n",
    "\n",
    "def get_geo_loss(gt_geo, pred_geo):\n",
    "    d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1)\n",
    "    d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1)\n",
    "    area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt)\n",
    "    area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred)\n",
    "    w_union = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred)\n",
    "    h_union = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred)\n",
    "    area_intersect = w_union * h_union\n",
    "    area_union = area_gt + area_pred - area_intersect\n",
    "    iou_loss_map = -torch.log((area_intersect + 1.0)/(area_union + 1.0))\n",
    "    angle_loss_map = 1 - torch.cos(angle_pred - angle_gt)\n",
    "    return iou_loss_map, angle_loss_map\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, weight_angle=10):\n",
    "        super(Loss, self).__init__()\n",
    "        self.weight_angle = weight_angle\n",
    "\n",
    "    def forward(self, gt_score, pred_score, gt_geo, pred_geo, ignored_map):\n",
    "        if torch.sum(gt_score) < 1:\n",
    "            return torch.sum(pred_score + pred_geo) * 0\n",
    "\n",
    "        classify_loss = get_dice_loss(gt_score, pred_score*(1-ignored_map))\n",
    "        iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo)\n",
    "\n",
    "        angle_loss = torch.sum(angle_loss_map*gt_score) / torch.sum(gt_score)\n",
    "        iou_loss = torch.sum(iou_loss_map*gt_score) / torch.sum(gt_score)\n",
    "        geo_loss = self.weight_angle * angle_loss + iou_loss\n",
    "        print('classify loss is {:.8f}, angle loss is {:.8f}, iou loss is {:.8f}'.format(classify_loss, angle_loss, iou_loss))\n",
    "        return geo_loss + classify_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8454fbe12c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mepoch_iter\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0msave_interval\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpths_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m# print(train_img_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8454fbe12c2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignored_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_geo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignored_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpred_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_geo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignored_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-119323174d6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-119323174d6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#explaination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 488\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################  train #########################\n",
    "\n",
    "\n",
    "def train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval):\n",
    "    file_num = len(os.listdir(train_img_path))\n",
    "    trainset = custom_dataset(train_img_path, train_gt_path)\n",
    "    train_loader = data.DataLoader(trainset, batch_size=batch_size, \\\n",
    "                                   shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "#     (img, gt_score, gt_geo, ignored_map) = train_loader\n",
    "#     print(img, gt_score, gt_geo, ignored_map)\n",
    "    criterion = Loss()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST()\n",
    "    data_parallel = False\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        data_parallel = True\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[epoch_iter//2], gamma=0.1)\n",
    "\n",
    "    for epoch in range(epoch_iter):\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        epoch_loss = 0\n",
    "        epoch_time = time.time()\n",
    "        for i, (img, gt_score, gt_geo, ignored_map) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            img, gt_score, gt_geo, ignored_map = img.to(device), gt_score.to(device), gt_geo.to(device), ignored_map.to(device)\n",
    "            pred_score, pred_geo = model(img)\n",
    "            loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('Epoch is [{}/{}], mini-batch is [{}/{}], time consumption is {:.8f}, batch_loss is {:.8f}'.format(\\\n",
    "              epoch+1, epoch_iter, i+1, int(file_num/batch_size), time.time()-start_time, loss.item()))\n",
    "\n",
    "        print('epoch_loss is {:.8f}, epoch_time is {:.8f}'.format(epoch_loss/int(file_num/batch_size), time.time()-epoch_time))\n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        print('='*50)\n",
    "#         if (epoch + 1) % interval == 0:\n",
    "#             state_dict = model.module.state_dict() if data_parallel else model.state_dict()\n",
    "#             torch.save(state_dict, os.path.join(pths_path, 'model_epoch_{}.pth'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------  DETECT -----------------------------------\n",
    "\n",
    "def resize_img(img):\n",
    "    '''resize image to be divisible by 32\n",
    "    '''\n",
    "    w, h = img.size\n",
    "    resize_w = w\n",
    "    resize_h = h\n",
    "\n",
    "    resize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\n",
    "    resize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\n",
    "    img = img.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "    ratio_h = resize_h / h\n",
    "    ratio_w = resize_w / w\n",
    "\n",
    "    return img, ratio_h, ratio_w\n",
    "\n",
    "\n",
    "def load_pil(img):\n",
    "    '''convert PIL Image to torch.Tensor\n",
    "    '''\n",
    "    t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "    return t(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def is_valid_poly(res, score_shape, scale):\n",
    "    '''check if the poly in image scope\n",
    "    Input:\n",
    "        res        : restored poly in original image\n",
    "        score_shape: score map shape\n",
    "        scale      : feature map -> image\n",
    "    Output:\n",
    "        True if valid\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for i in range(res.shape[1]):\n",
    "        if res[0,i] < 0 or res[0,i] >= score_shape[1] * scale or \\\n",
    "           res[1,i] < 0 or res[1,i] >= score_shape[0] * scale:\n",
    "            cnt += 1\n",
    "    return True if cnt <= 1 else False\n",
    "\n",
    "\n",
    "def restore_polys(valid_pos, valid_geo, score_shape, scale=4):\n",
    "    '''restore polys from feature maps in given positions\n",
    "    Input:\n",
    "        valid_pos  : potential text positions <numpy.ndarray, (n,2)>\n",
    "        valid_geo  : geometry in valid_pos <numpy.ndarray, (5,n)>\n",
    "        score_shape: shape of score map\n",
    "        scale      : image / feature map\n",
    "    Output:\n",
    "        restored polys <numpy.ndarray, (n,8)>, index\n",
    "    '''\n",
    "    polys = []\n",
    "    index = []\n",
    "    valid_pos *= scale\n",
    "    d = valid_geo[:4, :] # 4 x N\n",
    "    angle = valid_geo[4, :] # N,\n",
    "\n",
    "    for i in range(valid_pos.shape[0]):\n",
    "        x = valid_pos[i, 0]\n",
    "        y = valid_pos[i, 1]\n",
    "        y_min = y - d[0, i]\n",
    "        y_max = y + d[1, i]\n",
    "        x_min = x - d[2, i]\n",
    "        x_max = x + d[3, i]\n",
    "        rotate_mat = get_rotate_mat(-angle[i])\n",
    "\n",
    "        temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\n",
    "        temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\n",
    "        coordidates = np.concatenate((temp_x, temp_y), axis=0)\n",
    "        res = np.dot(rotate_mat, coordidates)\n",
    "        res[0,:] += x\n",
    "        res[1,:] += y\n",
    "\n",
    "        if is_valid_poly(res, score_shape, scale):\n",
    "            index.append(i)\n",
    "            polys.append([res[0,0], res[1,0], res[0,1], res[1,1], res[0,2], res[1,2],res[0,3], res[1,3]])\n",
    "    return np.array(polys), index\n",
    "\n",
    "\n",
    "def get_boxes(score, geo, score_thresh=0.9, nms_thresh=0.2):\n",
    "    '''get boxes from feature map\n",
    "    Input:\n",
    "        score       : score map from model <numpy.ndarray, (1,row,col)>\n",
    "        geo         : geo map from model <numpy.ndarray, (5,row,col)>\n",
    "        score_thresh: threshold to segment score map\n",
    "        nms_thresh  : threshold in nms\n",
    "    Output:\n",
    "        boxes       : final polys <numpy.ndarray, (n,9)>\n",
    "    '''\n",
    "    score = score[0,:,:]\n",
    "    xy_text = np.argwhere(score > score_thresh) # n x 2, format is [r, c]\n",
    "    if xy_text.size == 0:\n",
    "        return None\n",
    "\n",
    "    xy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
    "    valid_pos = xy_text[:, ::-1].copy() # n x 2, [x, y]\n",
    "    valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]] # 5 x n\n",
    "    polys_restored, index = restore_polys(valid_pos, valid_geo, score.shape) \n",
    "    if polys_restored.size == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n",
    "    boxes[:, :8] = polys_restored\n",
    "    boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n",
    "    boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thresh)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def adjust_ratio(boxes, ratio_w, ratio_h):\n",
    "    '''refine boxes\n",
    "    Input:\n",
    "        boxes  : detected polys <numpy.ndarray, (n,9)>\n",
    "        ratio_w: ratio of width\n",
    "        ratio_h: ratio of height\n",
    "    Output:\n",
    "        refined boxes\n",
    "    '''\n",
    "    if boxes is None or boxes.size == 0:\n",
    "        return None\n",
    "    boxes[:,[0,2,4,6]] /= ratio_w\n",
    "    boxes[:,[1,3,5,7]] /= ratio_h\n",
    "    return np.around(boxes)\n",
    "\n",
    "\n",
    "def detect(img, model, device):\n",
    "    '''detect text regions of img using model\n",
    "    Input:\n",
    "        img   : PIL Image\n",
    "        model : detection model\n",
    "        device: gpu if gpu is available\n",
    "    Output:\n",
    "        detected polys\n",
    "    '''\n",
    "    img, ratio_h, ratio_w = resize_img(img)\n",
    "    with torch.no_grad():\n",
    "        score, geo = model(load_pil(img).to(device))\n",
    "    boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n",
    "    return adjust_ratio(boxes, ratio_w, ratio_h)\n",
    "\n",
    "\n",
    "def plot_boxes(img, boxes):\n",
    "    '''plot boxes on image\n",
    "    '''\n",
    "    if boxes is None:\n",
    "        return img\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        draw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0,255,0))\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect_dataset(model, device, test_img_path, submit_path):\n",
    "    '''detection on whole dataset, save .txt results in submit_path\n",
    "    Input:\n",
    "        model        : detection model\n",
    "        device       : gpu if gpu is available\n",
    "        test_img_path: dataset path\n",
    "        submit_path  : submit result for evaluation\n",
    "    '''\n",
    "    img_files = os.listdir(test_img_path)\n",
    "    img_files = sorted([os.path.join(test_img_path, img_file) for img_file in img_files])\n",
    "\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        print('evaluating {} image'.format(i), end='\\r')\n",
    "        boxes = detect(Image.open(img_file), model, device)\n",
    "        seq = []\n",
    "        if boxes is not None:\n",
    "            seq.extend([','.join([str(int(b)) for b in box[:-1]]) + '\\n' for box in boxes])\n",
    "        with open(os.path.join(submit_path, 'res_' + os.path.basename(img_file).replace('.jpg','.txt')), 'w') as f:\n",
    "            f.writelines(seq)\n",
    "#     return boxes\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     img_path    = '../ICDAR_2015/test_img/img_2.jpg'\n",
    "#     model_path  = './pths/east_vgg16.pth'\n",
    "#     res_img     = './res.bmp'\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = EAST().to(device)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     img = Image.open(img_path)\n",
    "\n",
    "#     boxes = detect(img, model, device)\n",
    "#     plot_img = plot_boxes(img, boxes)\n",
    "#     plot_img.save(res_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval here to draw boxes around pics and show them but If i want to make testing code to calc criterion (loss)\n",
    "# and accuracy, make like train with no grad\n",
    "\n",
    "def eval_model(model_name, test_img_path, submit_path, save_flag=True):\n",
    "    if os.path.exists(submit_path):\n",
    "        shutil.rmtree(submit_path) \n",
    "    os.mkdir(submit_path)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST(False).to(device)\n",
    "#     model.load_state_dict(torch.load(model_name))\n",
    "    model.eval()  #stop calc back prop\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    detect_dataset(model, device, test_img_path, submit_path)\n",
    "    boxes = detect_dataset(model, device, test_img_path, submit_path)\n",
    "#     code of pytesseract\n",
    "\n",
    "\n",
    "#     os.chdir(submit_path)\n",
    "#     res = subprocess.getoutput('zip -q submit.zip *.txt')\n",
    "#     res = subprocess.getoutput('mv submit.zip ../')\n",
    "#     os.chdir('../')\n",
    "#     res = subprocess.getoutput('python ./evaluate/script.py –g=./evaluate/gt.zip –s=./submit.zip')\n",
    "#     print(res)\n",
    "#     os.remove('./submit.zip')\n",
    "    print('eval time is {}'.format(time.time()-start_time))\n",
    "\n",
    "    if not save_flag:\n",
    "        shutil.rmtree(submit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "#     initialize variables\n",
    "\n",
    "    train_img_path = os.path.abspath('ds/new train')\n",
    "    train_gt_path  = os.path.abspath('ds/new train gt')\n",
    "    pths_path      = './pths'\n",
    "    batch_size     = 24 \n",
    "    lr             = 1e-3\n",
    "    num_workers    = 4\n",
    "    epoch_iter     = 600\n",
    "    save_interval  = 5\n",
    "    model_name = './pths/east_vgg16.pth'\n",
    "    test_img_path = os.path.abspath('/home/ahmed/intern work/ocr/ds/new test')\n",
    "    submit_path = '/home/ahmed/intern work/ocr/ds/submit'\n",
    "#     train model\n",
    "    print(\"start training...........\")\n",
    "    train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, save_interval)\n",
    "#     test model\n",
    "    print()\n",
    "    print()\n",
    "    print(\"start testing...........\")\n",
    "    eval_model(model_name, test_img_path, submit_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting (img_path):\n",
    "    \n",
    "    model_path  = './pths/east_vgg16.pth'\n",
    "    res_img     = 'drive/My Drive/Colab Notebooks/ds'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST().to(device)\n",
    "    # model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    boxes = detect(img, model, device)\n",
    "    plot_img = plot_boxes(img, boxes)\n",
    "    plot_img.save(res_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n",
    "plotting('content/drive/My Drive/Colab Notebooks/ds/new test/img_498.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################  text recognition ###################\n",
    "\n",
    "#-----------------------------------------------\n",
    "# when making testing code, put this with it.  |\n",
    "#-----------------------------------------------\n",
    "\n",
    "# initialize the list of results\n",
    "results = []\n",
    "\n",
    "# loop over the bounding boxes to find the coordinate of bounding boxes\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "    # scale the coordinates based on the respective ratios in order to reflect bounding box on the original image\n",
    "    startX = int(startX * rW)\n",
    "    startY = int(startY * rH)\n",
    "    endX = int(endX * rW)\n",
    "    endY = int(endY * rH)\n",
    "\n",
    "    #extract the region of interest\n",
    "    r = orig[startY:endY, startX:endX]\n",
    "\n",
    "    #configuration setting to convert image to string.  \n",
    "    configuration = (\"-l eng --oem 1 --psm 8\")\n",
    "    ##This will recognize the text from the image of bounding box\n",
    "    text = pytesseract.image_to_string(r, config=configuration)\n",
    "\n",
    "    # append bbox coordinate and associated text to the list of results \n",
    "    results.append(((startX, startY, endX, endY), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing image after blobing it\n",
    "img = cv2.imread('ds/new train/img_1.jpg')\n",
    "blob = cv2.dnn.blobFromImage(img,1.0,(300,300),[104,117,123],False,False)\n",
    "print(blob.shape)\n",
    "blobb = blob.reshape(blob.shape[2],blob.shape[3],blob.shape[1])\n",
    "cv2.imshow('Blob',blobb)\n",
    "cv2.waitKey(15000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
